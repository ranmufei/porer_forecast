"""
æµ‹è¯•ä¸åŒé›†æˆæƒé‡ç»„åˆï¼Œå¯»æ‰¾æœ€ä¼˜é…ç½®
"""

import pickle
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import sys

print('=' * 70)
print('é›†æˆæƒé‡ä¼˜åŒ–æµ‹è¯•')
print('=' * 70)

# åŠ è½½æ•°æ®
print('\næ­£åœ¨åŠ è½½æ•°æ®...')
gfs_df = pd.read_csv('data_gfs_forecast.csv')
power_df = pd.read_csv('data_history_power.csv', header=None, skiprows=1, names=['timestamp', 'power'])

# å¯¼å…¥ç‰¹å¾å·¥ç¨‹
from train_power_forecast_optimized import feature_engineering

merged_df = pd.merge(gfs_df, power_df, left_index=True, right_index=True)
featured_df = feature_engineering(merged_df)

# å‡†å¤‡æµ‹è¯•é›†ç‰¹å¾ï¼ˆä½¿ç”¨æ–°çš„29ä¸ªç‰¹å¾ï¼‰
features_to_use = [
    'gfs_temp', 'gfs_wind_speed', 'gfs_wind_direction',
    'hour', 'day', 'month', 'day_of_week', 'day_of_year',
    'hour_sin', 'hour_cos', 'wind_dir_sin', 'wind_dir_cos',
    'wind_speed_square', 'wind_speed_cube', 'wind_speed_power_2.5',
    'wind_3_5', 'wind_5_10', 'wind_10_15', 'wind_15_25',
    'wind_saturation', 'estimated_power_limit', 'wind_stability',
    'seasonal_efficiency',
    'temp_change', 'temp_change_abs', 'temp_rolling_mean_3',
    'gfs_temp_normalized', 'temp_0_15', 'temp_15_25'
]

split_idx = int(len(featured_df) * 0.8)
X_test = featured_df[features_to_use].iloc[split_idx:]
y_test = featured_df['power'].iloc[split_idx:]

print(f'æµ‹è¯•é›†å¤§å°: {len(X_test)}')
print(f'ç‰¹å¾æ•°é‡: {len(features_to_use)}')

# åŠ è½½æ¨¡å‹
print('\næ­£åœ¨åŠ è½½æ¨¡å‹...')
try:
    with open('power_forecast_model_optimized_xgboost.pkl', 'rb') as f:
        xgb_model = pickle.load(f)
    with open('power_forecast_model_optimized_random_forest.pkl', 'rb') as f:
        rf_model = pickle.load(f)
    with open('power_forecast_model_optimized_gradient_boosting.pkl', 'rb') as f:
        gb_model = pickle.load(f)
    print('âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ')
except Exception as e:
    print(f'âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {e}')
    sys.exit(1)

# å„æ¨¡å‹é¢„æµ‹
print('\næ­£åœ¨ç”Ÿæˆé¢„æµ‹...')
pred_xgb = xgb_model.predict(X_test)
pred_rf = rf_model.predict(X_test)
pred_gb = gb_model.predict(X_test)

# å•æ¨¡å‹æ€§èƒ½
print('\n' + '=' * 70)
print('å•æ¨¡å‹æ€§èƒ½åŸºå‡†')
print('=' * 70)

mae_xgb = mean_absolute_error(y_test, pred_xgb)
mae_rf = mean_absolute_error(y_test, pred_rf)
mae_gb = mean_absolute_error(y_test, pred_gb)

print(f'\nXGBoost (å•ç‹¬):')
print(f'  MAE:  {mae_xgb:.2f} kW')
print(f'  RMSE: {np.sqrt(mean_squared_error(y_test, pred_xgb)):.2f} kW')
print(f'  RÂ²:   {r2_score(y_test, pred_xgb):.4f}')

print(f'\nRandom Forest (å•ç‹¬):')
print(f'  MAE:  {mae_rf:.2f} kW')
print(f'  RMSE: {np.sqrt(mean_squared_error(y_test, pred_rf)):.2f} kW')
print(f'  RÂ²:   {r2_score(y_test, pred_rf):.4f}')

print(f'\nGradient Boosting (å•ç‹¬):')
print(f'  MAE:  {mae_gb:.2f} kW')
print(f'  RMSE: {np.sqrt(mean_squared_error(y_test, pred_gb)):.2f} kW')
print(f'  RÂ²:   {r2_score(y_test, pred_gb):.4f}')

# æµ‹è¯•ä¸åŒæƒé‡ç»„åˆ
print('\n' + '=' * 70)
print('é›†æˆæƒé‡ç»„åˆæµ‹è¯•')
print('=' * 70)

weight_configs = [
    # (XGB, RF, GB, åç§°)
    (0.7, 0.2, 0.1, 'å½“å‰é…ç½® (0.7, 0.2, 0.1)'),
    (0.8, 0.1, 0.1, 'æ–¹æ¡ˆ1 (0.8, 0.1, 0.1)'),
    (0.9, 0.05, 0.05, 'æ–¹æ¡ˆ2 (0.9, 0.05, 0.05)'),
    (1.0, 0.0, 0.0, 'æ–¹æ¡ˆ3 - XGBoostç‹¬å¥'),
    (0.6, 0.3, 0.1, 'æ–¹æ¡ˆ4 (0.6, 0.3, 0.1)'),
    (0.5, 0.4, 0.1, 'æ–¹æ¡ˆ5 (0.5, 0.4, 0.1)'),
    (0.8, 0.15, 0.05, 'æ–¹æ¡ˆ6 (0.8, 0.15, 0.05)'),
    (0.85, 0.1, 0.05, 'æ–¹æ¡ˆ7 (0.85, 0.1, 0.05)'),
]

results = []
current_mae = None

print(f'\n{"é…ç½®":<35} {"MAE (kW)":>12} {"RMSE (kW)":>12} {"RÂ²":>8} {"æ”¹å–„":>10}')
print('-' * 70)

for w_xgb, w_rf, w_gb, name in weight_configs:
    # åŠ æƒé›†æˆ
    ensemble_pred = w_xgb * pred_xgb + w_rf * pred_rf + w_gb * pred_gb

    # è®¡ç®—æŒ‡æ ‡
    mae = mean_absolute_error(y_test, ensemble_pred)
    rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))
    r2 = r2_score(y_test, ensemble_pred)

    results.append({
        'name': name,
        'weights': (w_xgb, w_rf, w_gb),
        'mae': mae,
        'rmse': rmse,
        'r2': r2
    })

    # æ ‡è®°å½“å‰é…ç½®
    is_current = '(0.7, 0.2, 0.1)' in name
    if is_current:
        current_mae = mae
        marker = ' â† å½“å‰'
    else:
        marker = ''

    # è®¡ç®—æ”¹å–„
    if current_mae and not is_current:
        improvement = (current_mae - mae) / current_mae * 100
        improvement_str = f'{improvement:+.2f}%'
    else:
        improvement_str = '-'

    print(f'{name:<35} {mae:>10.2f}  {rmse:>10.2f}  {r2:>6.4f}  {improvement_str:>9}{marker}')

# æ‰¾å‡ºæœ€ä½³é…ç½®
print('\n' + '=' * 70)
print('æœ€ä½³é…ç½®æ¨è')
print('=' * 70)

best = min(results, key=lambda x: x['mae'])
print(f'\nğŸ† æœ€ä½³MAE: {best["name"]}')
print(f'   æƒé‡: XGBoost={best["weights"][0]}, RF={best["weights"][1]}, GB={best["weights"][2]}')
print(f'   MAE:  {best["mae"]:.2f} kW')
print(f'   RMSE: {best["rmse"]:.2f} kW')
print(f'   RÂ²:   {best["r2"]:.4f}')

if current_mae:
    improvement = (current_mae - best['mae']) / current_mae * 100
    print(f'   ç›¸æ¯”å½“å‰æ”¹å–„: {improvement:.2f}%')

# RMSEæœ€ä½³
best_rmse = min(results, key=lambda x: x['rmse'])
print(f'\nğŸ¥ˆ æœ€ä½³RMSE: {best_rmse["name"]}')
print(f'   RMSE: {best_rmse["rmse"]:.2f} kW')

# ä¿å­˜æ¨èé…ç½®åˆ°æ–‡ä»¶
print('\n' + '=' * 70)
print('ä¿å­˜æ¨èé…ç½®')
print('=' * 70)

recommended_weights = {
    'xgboost': float(best['weights'][0]),
    'random_forest': float(best['weights'][1]),
    'gradient_boosting': float(best['weights'][2]),
    'mae': float(best['mae']),
    'rmse': float(best['rmse']),
    'r2': float(best['r2']),
    'config_name': best['name']
}

with open('recommended_weights.pkl', 'wb') as f:
    pickle.dump(recommended_weights, f)

print(f'\nâœ“ æ¨èé…ç½®å·²ä¿å­˜è‡³: recommended_weights.pkl')
print(f'  é…ç½®: {best["name"]}')
print(f'  åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨æ­¤æƒé‡ä»¥è·å¾—æœ€ä½³æ€§èƒ½')

print('\n' + '=' * 70)
print('æµ‹è¯•å®Œæˆ')
print('=' * 70)
